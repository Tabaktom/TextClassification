{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "ZHyESj7XQboj",
    "outputId": "26e01dfa-6b94-4b6b-c3d9-59932b3d3075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "multilabel guide: {'OFF-TIN-OTH': 0, 'OFF-TIN-IND': 1, 'OFF-TIN-GRP': 2, 'OFF-UNT-NULL': 3, 'NOT-NULL-NULL': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import string\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import keras\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "from keras.callbacks import EarlyStopping\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def read_file(path):\n",
    "  dataset = []\n",
    "  with open(path) as csvfile:\n",
    "      file = csv.reader(csvfile, delimiter='\\t')\n",
    "      next(file)\n",
    "      for row in file:\n",
    "        dataset.append(row)\n",
    "  return dataset\n",
    "\n",
    "df_pre = pd.DataFrame(read_file('/content/olid-training-v1.0.tsv'), \n",
    "                      columns = ['ID', 'tweet', 'A', 'B', 'C'])\n",
    "df_pre['Combined'] = df_pre['A']+'-' + df_pre['B'] +'-'+df_pre['C']\n",
    "multilabels = {lab:ind for ind, lab in enumerate(set(df_pre['Combined'].values.tolist()))}\n",
    "print('multilabel guide:', multilabels)\n",
    "\n",
    "levela= pd.read_csv('/content/testset-levela.tsv',  sep='\\t')\n",
    "levelb =pd.read_csv('/content/testset-levelb.tsv', sep='\\t')\n",
    "levelc = pd.read_csv('/content/testset-levelc.tsv', sep='\\t')\n",
    "\n",
    "levela_labels=pd.read_csv('/content/labels-levela.csv', names=['id', 'A'])\n",
    "levelb_labels=pd.read_csv('/content/labels-levelb.csv',  names=['id', 'B'])\n",
    "levelc_labels=pd.read_csv('/content/labels-levelc.csv', names=['id', 'C'])\n",
    "\n",
    "Alevel = pd.merge(levela, levela_labels, on='id')\n",
    "Blevel = pd.merge(levelb, levelb_labels, on='id')\n",
    "Clevel = pd.merge(levelc, levelc_labels, on='id')\n",
    "\n",
    "categorical_test = pd.merge(left=Alevel, right=Blevel, how='left', on=['id', 'tweet'])\n",
    "categorical_test = pd.merge(left=categorical_test, right=Clevel, how='left', on=['id', 'tweet'])\n",
    "categorical_test = categorical_test.fillna('NULL')\n",
    "categorical_test['Combined'] = categorical_test['A']+'-' + categorical_test['B'] +'-'+categorical_test['C']\n",
    "for ind, row in categorical_test.iterrows():\n",
    "  categorical_test['Combined'][ind] = multilabels[row.Combined]\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "def preprocess(word_string, emojy = False):\n",
    "  if emojy==False:\n",
    "    word_string = deEmojify(word_string)\n",
    "  lower_words = text_to_word_sequence(word_string, filters=string.punctuation, lower = False, split=' ')\n",
    "  Fsentences = [] \n",
    "  for word in lower_words:\n",
    "    words = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', word)).split() \n",
    "    for w in words:\n",
    "      if w not in stop_words and not w.isdigit():\n",
    "        Fsentences.append(w.lower())\n",
    "  length = len(Fsentences)\n",
    "  return Fsentences, length\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T21LrZRe7EnV"
   },
   "source": [
    "Level A label guide: 0 - OFF, 1 -NOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjIhdobAbtlc"
   },
   "source": [
    "## Preprocessing of Text Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "id": "6aqo3vUCbpD-",
    "outputId": "a7949262-c04e-45b5-c368-6d3611cad042"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, MultiLabelBinarizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "'''\n",
    "Preprocessing of tweets and finding number of tokens in longest tweet\n",
    "'''\n",
    "def preprocessing_execution(df_pre, emojy=False):\n",
    "  max_length = 0\n",
    "  for ind, row in df_pre.iterrows():\n",
    "    df_pre['tweet'].iloc[ind],length= preprocess(row.tweet, emojy)\n",
    "    if length >max_length:\n",
    "      max_length=length \n",
    "  return df_pre, max_length\n",
    "\n",
    "df_pre, max_length = preprocessing_execution(df_pre, emojy=False)\n",
    "Alevel, _ = preprocessing_execution(Alevel, emojy=False)\n",
    "categorical_test,_ = preprocessing_execution(categorical_test, emojy=False)\n",
    "\n",
    "\n",
    "'''\n",
    "converting string categorical labels to numerical values\n",
    "'''\n",
    "\n",
    "def encode_labels(cols, df):\n",
    "  for col in cols:\n",
    "    labels = df.pop(col)\n",
    "    label_set = set(labels.values)\n",
    "    label_set_ids = {lab: id for id, lab in enumerate(label_set)}\n",
    "    final_labels = [label_set_ids[lab] for lab in labels]\n",
    "    df[col] = final_labels\n",
    "  return df \n",
    "\n",
    "Alevel = encode_labels(['A'],Alevel)\n",
    "df_pre = encode_labels(['A'], df_pre)\n",
    "df_pre = encode_labels(['Combined'], df_pre)\n",
    "\n",
    "le = LabelEncoder()\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(range(max(Alevel['A'].values.tolist())+1))\n",
    "Alevel['A'] = lb.transform(Alevel['A'])\n",
    "df_pre = encode_labels(['Combined'], df_pre)\n",
    "categorical_test_df = encode_labels(['Combined'], categorical_test)\n",
    "\n",
    "'''\n",
    "Creating a dictionary for the vocabulary to be used for word embedding\n",
    "'''\n",
    "text=df_pre['tweet'].values\n",
    "vocab = []\n",
    "for tweet in text:\n",
    "  for word in tweet:\n",
    "    vocab.append(word)\n",
    "vocab = set(vocab)\n",
    "vocab_dict = {}\n",
    "for ind, word in enumerate(vocab):\n",
    "  vocab_dict[word] = ind+2\n",
    "\n",
    "#Reserving Tokens for special purposes\n",
    "vocab_dict[\"<PAD>\"] = 0 \n",
    "vocab_dict[\"<UNK>\"] = 1  \n",
    "vocab_dict[\"<UNUSED>\"] = 2\n",
    "\n",
    "embedded_tweets=[]\n",
    "for tweet in text:\n",
    "  tw = []\n",
    "  for word in tweet:\n",
    "    tw.append(vocab_dict[word])\n",
    "  embedded_tweets.append(tw)\n",
    "df_pre['tweet'] = embedded_tweets\n",
    "\n",
    "def test_embeddings(df, vocab_dict):\n",
    "  embeddings =[]\n",
    "  for tweet in df['tweet'].values:\n",
    "    tw = []\n",
    "    for token in tweet:\n",
    "      if token not in list(vocab_dict.keys()):\n",
    "        tw.append(vocab_dict[\"<UNK>\"])\n",
    "      else:\n",
    "        tw.append(vocab_dict[token])\n",
    "    embeddings.append(tw)\n",
    "  return embeddings\n",
    "\n",
    "levela_test = test_embeddings(Alevel, vocab_dict)\n",
    "categorical_test = test_embeddings(categorical_test, vocab_dict)\n",
    "\n",
    "\n",
    "EMBED_SIZE=max_length*2\n",
    "data_train = pad_sequences(df_pre['tweet'].values, maxlen=EMBED_SIZE)\n",
    "A_test = pad_sequences(levela_test, maxlen=EMBED_SIZE)\n",
    "Categorical_test = pad_sequences(categorical_test, maxlen=EMBED_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53MI8_egeEJg"
   },
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = data_train, df_pre.A.values\n",
    "Ytest =Alevel.A.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Po3PfgcKmYKq"
   },
   "source": [
    "# Level A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_zqLWBCfCPV"
   },
   "source": [
    "### Setting of a Baseline using a linear Support Vector Machine (SVM) - Level A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "PtIiIo8BdSz_",
    "outputId": "5742767c-2708-4246-a12e-9a6b1c4d9ef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84       620\n",
      "           1       1.00      0.00      0.01       240\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.86      0.50      0.42       860\n",
      "weighted avg       0.80      0.72      0.61       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(Xtrain[:int(len(Xtrain)*0.8)], Ytrain[:int(len(Ytrain)*0.8)])\n",
    "#train_hat = svm_classifier.predict(data_train)\n",
    "test_hat = svm_classifier.predict(A_test)\n",
    "print('Test: \\n')\n",
    "print(classification_report(Ytest, test_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gz0228_ofW2a"
   },
   "source": [
    "### Model 1: Embedding and Global Pooling layers \n",
    "\n",
    "---\n",
    "\n",
    "(Part C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "-sQDS6JfzFhz",
    "outputId": "83ab9e89-8fa5-43a6-abad-41913db1b96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 162, 100)          1821800   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_mas (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,821,901\n",
      "Trainable params: 1,821,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
    "    def call(self, x, mask=None):\n",
    "        if mask != None:\n",
    "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "        else:\n",
    "            return super().call(x)\n",
    "\n",
    "VOCAB_SIZE = len(vocab_dict.items())\n",
    "def Embedding_pooling_model():\n",
    "  model=keras.Sequential()\n",
    "  model.add(keras.layers.Embedding(VOCAB_SIZE, 100, input_length=EMBED_SIZE))\n",
    "  model.add(GlobalAveragePooling1DMasked())\n",
    "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "  model.summary()\n",
    "  model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "embedding_modelA = Embedding_pooling_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TJu-rqJFfTaI",
    "outputId": "16910c4b-1d8b-4867-8ee2-b5850fb3b383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "10592/10592 [==============================] - 11s 1ms/step - loss: 0.6371 - acc: 0.6674 - val_loss: 0.6304 - val_acc: 0.6688\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 2s 201us/step - loss: 0.6273 - acc: 0.6674 - val_loss: 0.6241 - val_acc: 0.6688\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 2s 200us/step - loss: 0.6156 - acc: 0.6674 - val_loss: 0.6132 - val_acc: 0.6688\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 2s 191us/step - loss: 0.5943 - acc: 0.6712 - val_loss: 0.5979 - val_acc: 0.6764\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 2s 193us/step - loss: 0.5633 - acc: 0.6964 - val_loss: 0.5786 - val_acc: 0.6922\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 2s 191us/step - loss: 0.5259 - acc: 0.7352 - val_loss: 0.5610 - val_acc: 0.7134\n",
      "Epoch 7/20\n",
      "10592/10592 [==============================] - 2s 196us/step - loss: 0.4866 - acc: 0.7723 - val_loss: 0.5488 - val_acc: 0.7153\n",
      "Epoch 8/20\n",
      "10592/10592 [==============================] - 2s 193us/step - loss: 0.4510 - acc: 0.7998 - val_loss: 0.5362 - val_acc: 0.7413\n",
      "Epoch 9/20\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.4202 - acc: 0.8214 - val_loss: 0.5291 - val_acc: 0.7508\n",
      "Epoch 10/20\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.3902 - acc: 0.8395 - val_loss: 0.5247 - val_acc: 0.7564\n",
      "Epoch 11/20\n",
      "10592/10592 [==============================] - 2s 194us/step - loss: 0.3630 - acc: 0.8558 - val_loss: 0.5266 - val_acc: 0.7666\n",
      "Epoch 12/20\n",
      "10592/10592 [==============================] - 2s 187us/step - loss: 0.3406 - acc: 0.8689 - val_loss: 0.5325 - val_acc: 0.7610\n",
      "Epoch 13/20\n",
      "10592/10592 [==============================] - 2s 197us/step - loss: 0.3196 - acc: 0.8788 - val_loss: 0.5306 - val_acc: 0.7677\n",
      "Epoch 14/20\n",
      "10592/10592 [==============================] - 2s 193us/step - loss: 0.3003 - acc: 0.8880 - val_loss: 0.5360 - val_acc: 0.7666\n",
      "Epoch 15/20\n",
      "10592/10592 [==============================] - 2s 191us/step - loss: 0.2841 - acc: 0.8938 - val_loss: 0.5386 - val_acc: 0.7674\n",
      "Epoch 16/20\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.2672 - acc: 0.8999 - val_loss: 0.5459 - val_acc: 0.7651\n",
      "Epoch 17/20\n",
      "10592/10592 [==============================] - 2s 193us/step - loss: 0.2550 - acc: 0.9069 - val_loss: 0.5515 - val_acc: 0.7651\n",
      "Epoch 18/20\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 0.2407 - acc: 0.9129 - val_loss: 0.5634 - val_acc: 0.7568\n"
     ]
    }
   ],
   "source": [
    "cb = EarlyStopping(monitor='val_acc', mode='auto', patience=5)\n",
    "history1 = embedding_modelA.fit(Xtrain, Ytrain, epochs=20,\n",
    "                                validation_split=0.2, batch_size=32,\n",
    "                                verbose=1, workers = 3, use_multiprocessing=True, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Xw_BO0OCz7l4",
    "outputId": "27338051-1811-4fa7-8036-9e29410ff6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       620\n",
      "           1       0.62      0.51      0.56       240\n",
      "\n",
      "    accuracy                           0.78       860\n",
      "   macro avg       0.72      0.69      0.70       860\n",
      "weighted avg       0.77      0.78      0.77       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test: \\n')\n",
    "test_hat = [round(x[0], 0) for x in embedding_modelA.predict(A_test)]\n",
    "print(classification_report(Ytest, test_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "7kiCbKNr0N4U",
    "outputId": "bd59b385-74fc-4fb5-cb2f-aa897b2ae418"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotting_history(history):\n",
    "  history_dict = history.history\n",
    "  acc = history_dict['acc']\n",
    "  val_acc = history_dict['val_acc']\n",
    "  loss = history_dict['loss']\n",
    "  val_loss = history_dict['val_loss']\n",
    "  epochs = range(1, len(acc) + 1)\n",
    "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "plotting_history(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhiiFGN8GbS"
   },
   "source": [
    "#### Embedding and Global Pooling with additional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VjVxAbha8Mw3",
    "outputId": "6c88994b-3e00-4355-932d-df654d97d60a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 162, 100)          1821800   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_mas (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,823,433\n",
      "Trainable params: 1,823,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "10592/10592 [==============================] - 3s 238us/step - loss: 0.6383 - acc: 0.6634 - val_loss: 0.6305 - val_acc: 0.6688\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 2s 206us/step - loss: 0.6273 - acc: 0.6674 - val_loss: 0.6233 - val_acc: 0.6688\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 2s 201us/step - loss: 0.6058 - acc: 0.6703 - val_loss: 0.5960 - val_acc: 0.6839\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 2s 203us/step - loss: 0.5173 - acc: 0.7489 - val_loss: 0.5361 - val_acc: 0.7387\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 2s 211us/step - loss: 0.3972 - acc: 0.8293 - val_loss: 0.5308 - val_acc: 0.7610\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 2s 210us/step - loss: 0.3286 - acc: 0.8669 - val_loss: 0.5614 - val_acc: 0.7651\n",
      "Epoch 7/20\n",
      "10592/10592 [==============================] - 2s 204us/step - loss: 0.2758 - acc: 0.8898 - val_loss: 0.5821 - val_acc: 0.7500\n",
      "Epoch 8/20\n",
      "10592/10592 [==============================] - 2s 199us/step - loss: 0.2419 - acc: 0.9059 - val_loss: 0.6006 - val_acc: 0.7613\n",
      "Epoch 9/20\n",
      "10592/10592 [==============================] - 2s 206us/step - loss: 0.2113 - acc: 0.9187 - val_loss: 0.6329 - val_acc: 0.7564\n",
      "Epoch 10/20\n",
      "10592/10592 [==============================] - 2s 205us/step - loss: 0.1914 - acc: 0.9285 - val_loss: 0.6673 - val_acc: 0.7564\n",
      "Epoch 11/20\n",
      "10592/10592 [==============================] - 2s 204us/step - loss: 0.1740 - acc: 0.9346 - val_loss: 0.7070 - val_acc: 0.7387\n"
     ]
    }
   ],
   "source": [
    "def Embedding_pooling_model_additional(act):\n",
    "  model=keras.Sequential()\n",
    "  model.add(keras.layers.Embedding(VOCAB_SIZE, 100, input_length=EMBED_SIZE))\n",
    "  model.add(GlobalAveragePooling1DMasked())\n",
    "  model.add(keras.layers.Dense(16, activation=act))\n",
    "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "  model.summary()\n",
    "  model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "embedding_model1R = Embedding_pooling_model_additional('relu')\n",
    "cb = EarlyStopping(monitor='val_acc', mode='auto', patience=5)\n",
    "historyR1 = embedding_model1R.fit(Xtrain, Ytrain, epochs=20,\n",
    "                                validation_split=0.2, batch_size=32,\n",
    "                                verbose=1, workers = 3, use_multiprocessing=True, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "HcXhmRYY9Y31",
    "outputId": "94d01660-a679-4754-9b4b-f7500a19205c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       620\n",
      "           1       0.58      0.54      0.56       240\n",
      "\n",
      "    accuracy                           0.76       860\n",
      "   macro avg       0.71      0.69      0.70       860\n",
      "weighted avg       0.76      0.76      0.76       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test: \\n')\n",
    "test_hat = [round(x[0], 0) for x in embedding_model1R.predict(A_test)]\n",
    "print(classification_report(Ytest, test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "H2AItgmf99JD",
    "outputId": "ddaeaa16-2b58-4689-d58f-22863259cd02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 162, 100)          1821800   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_mas (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,823,433\n",
      "Trainable params: 1,823,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "10592/10592 [==============================] - 3s 243us/step - loss: 0.6346 - acc: 0.6674 - val_loss: 0.6300 - val_acc: 0.6688\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 2s 211us/step - loss: 0.6153 - acc: 0.6693 - val_loss: 0.6006 - val_acc: 0.6809\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 2s 203us/step - loss: 0.5290 - acc: 0.7382 - val_loss: 0.5360 - val_acc: 0.7443\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 2s 200us/step - loss: 0.4007 - acc: 0.8270 - val_loss: 0.5520 - val_acc: 0.7538\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 2s 200us/step - loss: 0.3189 - acc: 0.8699 - val_loss: 0.5672 - val_acc: 0.7564\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 2s 202us/step - loss: 0.2671 - acc: 0.8927 - val_loss: 0.6108 - val_acc: 0.7670\n",
      "Epoch 7/20\n",
      "10592/10592 [==============================] - 2s 207us/step - loss: 0.2205 - acc: 0.9140 - val_loss: 0.6556 - val_acc: 0.7583\n",
      "Epoch 8/20\n",
      "10592/10592 [==============================] - 2s 203us/step - loss: 0.1920 - acc: 0.9241 - val_loss: 0.7091 - val_acc: 0.7557\n",
      "Epoch 9/20\n",
      "10592/10592 [==============================] - 2s 207us/step - loss: 0.1653 - acc: 0.9371 - val_loss: 0.7696 - val_acc: 0.7474\n",
      "Epoch 10/20\n",
      "10592/10592 [==============================] - 2s 207us/step - loss: 0.1425 - acc: 0.9475 - val_loss: 0.8345 - val_acc: 0.7538\n",
      "Epoch 11/20\n",
      "10592/10592 [==============================] - 2s 212us/step - loss: 0.1231 - acc: 0.9555 - val_loss: 0.9012 - val_acc: 0.7417\n"
     ]
    }
   ],
   "source": [
    "embedding_model1T = Embedding_pooling_model_additional('tanh')\n",
    "cb = EarlyStopping(monitor='val_acc', mode='auto', patience=5)\n",
    "historyT1 = embedding_model1T.fit(Xtrain, Ytrain, epochs=20,\n",
    "                                validation_split=0.2, batch_size=32,\n",
    "                                verbose=1, workers = 3, use_multiprocessing=True, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2amhvymD-aGk",
    "outputId": "ec0e832f-5cd7-4caf-fcdb-010dd8704df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       620\n",
      "           1       0.58      0.50      0.54       240\n",
      "\n",
      "    accuracy                           0.76       860\n",
      "   macro avg       0.70      0.68      0.69       860\n",
      "weighted avg       0.75      0.76      0.75       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test: \\n')\n",
    "test_hat = [round(x[0], 0) for x in embedding_model1T.predict(A_test)]\n",
    "print(classification_report(Ytest, test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHQdPGpej085"
   },
   "source": [
    "# Multiclass - combined Levels A, B and C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8wT9qX1j5gw"
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "id": "8Nkc_Uxsj3zq",
    "outputId": "9456f8bb-ac92-401f-f693-a17d167edab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        35\n",
      "           1       0.00      0.00      0.00       100\n",
      "           2       0.00      0.00      0.00        78\n",
      "           3       0.00      0.00      0.00        27\n",
      "           4       0.72      1.00      0.84       620\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.14      0.20      0.17       860\n",
      "weighted avg       0.52      0.72      0.60       860\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain = data_train, df_pre.Combined.values\n",
    "Ytest =categorical_test_df['Combined'].values.tolist()\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(data_train[:int(len(data_train)*0.8)], Ytrain[:int(len(Ytrain)*0.8)])\n",
    "test_hat = svm_classifier.predict(Categorical_test)\n",
    "print('Test: \\n')\n",
    "print(classification_report(Ytest, test_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7ngiBE1HJcB"
   },
   "source": [
    "### Multiclass - Embedding pooling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bufiEQ5SmEnm",
    "outputId": "5b958b3c-1c49-4f28-e504-ef049fcc7958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 162, 100)          1821800   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_mas (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,823,501\n",
      "Trainable params: 1,823,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/100\n",
      "10592/10592 [==============================] - 2s 229us/step - loss: 7.3234 - acc: 0.1812 - val_loss: 6.8871 - val_acc: 0.1805\n",
      "Epoch 2/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 6.8930 - acc: 0.1821 - val_loss: 6.8600 - val_acc: 0.1805\n",
      "Epoch 3/100\n",
      "10592/10592 [==============================] - 2s 187us/step - loss: 6.8152 - acc: 0.1821 - val_loss: 6.8161 - val_acc: 0.1805\n",
      "Epoch 4/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 6.6557 - acc: 0.1821 - val_loss: 6.7266 - val_acc: 0.1805\n",
      "Epoch 5/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 6.3051 - acc: 0.1834 - val_loss: 6.6231 - val_acc: 0.1839\n",
      "Epoch 6/100\n",
      "10592/10592 [==============================] - 2s 197us/step - loss: 5.7663 - acc: 0.1999 - val_loss: 6.8205 - val_acc: 0.1922\n",
      "Epoch 7/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 5.2779 - acc: 0.2203 - val_loss: 6.7800 - val_acc: 0.1956\n",
      "Epoch 8/100\n",
      "10592/10592 [==============================] - 2s 196us/step - loss: 4.8768 - acc: 0.2453 - val_loss: 7.4617 - val_acc: 0.1952\n",
      "Epoch 9/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 4.4867 - acc: 0.2852 - val_loss: 7.6201 - val_acc: 0.2281\n",
      "Epoch 10/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 4.1585 - acc: 0.3398 - val_loss: 7.9230 - val_acc: 0.2749\n",
      "Epoch 11/100\n",
      "10592/10592 [==============================] - 2s 185us/step - loss: 3.8455 - acc: 0.3901 - val_loss: 8.4091 - val_acc: 0.2727\n",
      "Epoch 12/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 3.5525 - acc: 0.4197 - val_loss: 9.2910 - val_acc: 0.3255\n",
      "Epoch 13/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 3.3120 - acc: 0.4628 - val_loss: 9.2474 - val_acc: 0.3100\n",
      "Epoch 14/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 3.0580 - acc: 0.5151 - val_loss: 9.7264 - val_acc: 0.3248\n",
      "Epoch 15/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 2.8425 - acc: 0.5556 - val_loss: 11.2442 - val_acc: 0.3674\n",
      "Epoch 16/100\n",
      "10592/10592 [==============================] - 2s 187us/step - loss: 2.6734 - acc: 0.5980 - val_loss: 10.4411 - val_acc: 0.3535\n",
      "Epoch 17/100\n",
      "10592/10592 [==============================] - 2s 184us/step - loss: 2.4971 - acc: 0.6382 - val_loss: 12.0853 - val_acc: 0.4116\n",
      "Epoch 18/100\n",
      "10592/10592 [==============================] - 2s 185us/step - loss: 2.2953 - acc: 0.6799 - val_loss: 14.2877 - val_acc: 0.3875\n",
      "Epoch 19/100\n",
      "10592/10592 [==============================] - 2s 193us/step - loss: 2.1477 - acc: 0.7103 - val_loss: 12.8648 - val_acc: 0.4505\n",
      "Epoch 20/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 1.9710 - acc: 0.7444 - val_loss: 12.7497 - val_acc: 0.4120\n",
      "Epoch 21/100\n",
      "10592/10592 [==============================] - 2s 192us/step - loss: 1.8621 - acc: 0.7613 - val_loss: 14.5636 - val_acc: 0.4807\n",
      "Epoch 22/100\n",
      "10592/10592 [==============================] - 2s 193us/step - loss: 1.7174 - acc: 0.7856 - val_loss: 15.1146 - val_acc: 0.4777\n",
      "Epoch 23/100\n",
      "10592/10592 [==============================] - 2s 192us/step - loss: 1.6311 - acc: 0.7951 - val_loss: 15.8624 - val_acc: 0.4713\n",
      "Epoch 24/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 1.4854 - acc: 0.8151 - val_loss: 16.8495 - val_acc: 0.4898\n",
      "Epoch 25/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 1.3948 - acc: 0.8302 - val_loss: 16.5567 - val_acc: 0.5310\n",
      "Epoch 26/100\n",
      "10592/10592 [==============================] - 2s 196us/step - loss: 1.3243 - acc: 0.8395 - val_loss: 15.7090 - val_acc: 0.4275\n",
      "Epoch 27/100\n",
      "10592/10592 [==============================] - 2s 191us/step - loss: 1.1977 - acc: 0.8549 - val_loss: 16.3189 - val_acc: 0.4751\n",
      "Epoch 28/100\n",
      "10592/10592 [==============================] - 2s 197us/step - loss: 1.1302 - acc: 0.8608 - val_loss: 18.3529 - val_acc: 0.5442\n",
      "Epoch 29/100\n",
      "10592/10592 [==============================] - 2s 195us/step - loss: 1.0701 - acc: 0.8737 - val_loss: 16.8114 - val_acc: 0.4520\n",
      "Epoch 30/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.9603 - acc: 0.8866 - val_loss: 19.4577 - val_acc: 0.5177\n",
      "Epoch 31/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 0.9008 - acc: 0.8958 - val_loss: 20.5131 - val_acc: 0.5566\n",
      "Epoch 32/100\n",
      "10592/10592 [==============================] - 2s 195us/step - loss: 0.8633 - acc: 0.8989 - val_loss: 20.2000 - val_acc: 0.5227\n",
      "Epoch 33/100\n",
      "10592/10592 [==============================] - 2s 197us/step - loss: 0.7942 - acc: 0.9090 - val_loss: 20.8829 - val_acc: 0.5283\n",
      "Epoch 34/100\n",
      "10592/10592 [==============================] - 2s 199us/step - loss: 0.7436 - acc: 0.9110 - val_loss: 22.3498 - val_acc: 0.5140\n",
      "Epoch 35/100\n",
      "10592/10592 [==============================] - 2s 191us/step - loss: 0.7381 - acc: 0.9113 - val_loss: 23.5533 - val_acc: 0.5687\n",
      "Epoch 36/100\n",
      "10592/10592 [==============================] - 2s 197us/step - loss: 0.6655 - acc: 0.9195 - val_loss: 23.8564 - val_acc: 0.5472\n",
      "Epoch 37/100\n",
      "10592/10592 [==============================] - 2s 191us/step - loss: 0.6401 - acc: 0.9200 - val_loss: 24.3808 - val_acc: 0.5982\n",
      "Epoch 38/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.6117 - acc: 0.9265 - val_loss: 24.1420 - val_acc: 0.5770\n",
      "Epoch 39/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.5808 - acc: 0.9307 - val_loss: 24.9140 - val_acc: 0.5797\n",
      "Epoch 40/100\n",
      "10592/10592 [==============================] - 2s 185us/step - loss: 0.5345 - acc: 0.9331 - val_loss: 27.7763 - val_acc: 0.6186\n",
      "Epoch 41/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.5234 - acc: 0.9367 - val_loss: 24.8503 - val_acc: 0.5483\n",
      "Epoch 42/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.4840 - acc: 0.9396 - val_loss: 24.8031 - val_acc: 0.5476\n",
      "Epoch 43/100\n",
      "10592/10592 [==============================] - 2s 192us/step - loss: 0.4947 - acc: 0.9407 - val_loss: 29.4846 - val_acc: 0.6001\n",
      "Epoch 44/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 0.4511 - acc: 0.9451 - val_loss: 25.6867 - val_acc: 0.5608\n",
      "Epoch 45/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.4598 - acc: 0.9464 - val_loss: 26.6543 - val_acc: 0.5415\n",
      "Epoch 46/100\n",
      "10592/10592 [==============================] - 2s 187us/step - loss: 0.4287 - acc: 0.9503 - val_loss: 27.3837 - val_acc: 0.5710\n",
      "Epoch 47/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 0.3921 - acc: 0.9526 - val_loss: 26.9877 - val_acc: 0.5582\n",
      "Epoch 48/100\n",
      "10592/10592 [==============================] - 2s 191us/step - loss: 0.3989 - acc: 0.9541 - val_loss: 27.5801 - val_acc: 0.5759\n",
      "Epoch 49/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.4067 - acc: 0.9523 - val_loss: 29.3160 - val_acc: 0.6012\n",
      "Epoch 50/100\n",
      "10592/10592 [==============================] - 2s 185us/step - loss: 0.3530 - acc: 0.9580 - val_loss: 30.5384 - val_acc: 0.6027\n",
      "Epoch 51/100\n",
      "10592/10592 [==============================] - 2s 186us/step - loss: 0.3528 - acc: 0.9583 - val_loss: 32.2240 - val_acc: 0.5823\n",
      "Epoch 52/100\n",
      "10592/10592 [==============================] - 2s 186us/step - loss: 0.3392 - acc: 0.9623 - val_loss: 28.6048 - val_acc: 0.5480\n",
      "Epoch 53/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.3445 - acc: 0.9616 - val_loss: 30.1715 - val_acc: 0.5752\n",
      "Epoch 54/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.3515 - acc: 0.9623 - val_loss: 29.2310 - val_acc: 0.5468\n",
      "Epoch 55/100\n",
      "10592/10592 [==============================] - 2s 186us/step - loss: 0.3196 - acc: 0.9645 - val_loss: 31.6116 - val_acc: 0.6020\n",
      "Epoch 56/100\n",
      "10592/10592 [==============================] - 2s 192us/step - loss: 0.3619 - acc: 0.9573 - val_loss: 29.4065 - val_acc: 0.5151\n",
      "Epoch 57/100\n",
      "10592/10592 [==============================] - 2s 192us/step - loss: 0.3045 - acc: 0.9641 - val_loss: 31.1220 - val_acc: 0.5736\n",
      "Epoch 58/100\n",
      "10592/10592 [==============================] - 2s 197us/step - loss: 0.2999 - acc: 0.9673 - val_loss: 31.3105 - val_acc: 0.5472\n",
      "Epoch 59/100\n",
      "10592/10592 [==============================] - 2s 194us/step - loss: 0.2658 - acc: 0.9692 - val_loss: 30.2190 - val_acc: 0.5472\n",
      "Epoch 60/100\n",
      "10592/10592 [==============================] - 2s 194us/step - loss: 0.2913 - acc: 0.9677 - val_loss: 33.6754 - val_acc: 0.6144\n",
      "Epoch 61/100\n",
      "10592/10592 [==============================] - 2s 193us/step - loss: 0.2760 - acc: 0.9687 - val_loss: 31.9399 - val_acc: 0.5714\n",
      "Epoch 62/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.2676 - acc: 0.9703 - val_loss: 35.8339 - val_acc: 0.6261\n",
      "Epoch 63/100\n",
      "10592/10592 [==============================] - 2s 193us/step - loss: 0.2762 - acc: 0.9727 - val_loss: 31.1540 - val_acc: 0.5517\n",
      "Epoch 64/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.2798 - acc: 0.9680 - val_loss: 35.4249 - val_acc: 0.6110\n",
      "Epoch 65/100\n",
      "10592/10592 [==============================] - 2s 195us/step - loss: 0.2478 - acc: 0.9723 - val_loss: 33.5517 - val_acc: 0.5702\n",
      "Epoch 66/100\n",
      "10592/10592 [==============================] - 2s 195us/step - loss: 0.2556 - acc: 0.9711 - val_loss: 36.3611 - val_acc: 0.6152\n",
      "Epoch 67/100\n",
      "10592/10592 [==============================] - 2s 192us/step - loss: 0.2350 - acc: 0.9742 - val_loss: 31.8342 - val_acc: 0.5366\n",
      "Epoch 68/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.2373 - acc: 0.9744 - val_loss: 31.4238 - val_acc: 0.5177\n",
      "Epoch 69/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.2398 - acc: 0.9737 - val_loss: 31.9326 - val_acc: 0.5295\n",
      "Epoch 70/100\n",
      "10592/10592 [==============================] - 2s 187us/step - loss: 0.2721 - acc: 0.9726 - val_loss: 32.2066 - val_acc: 0.5272\n",
      "Epoch 71/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.2568 - acc: 0.9732 - val_loss: 34.9436 - val_acc: 0.5785\n",
      "Epoch 72/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.2229 - acc: 0.9754 - val_loss: 33.6707 - val_acc: 0.5287\n",
      "Epoch 73/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 0.2681 - acc: 0.9678 - val_loss: 37.7833 - val_acc: 0.5857\n",
      "Epoch 74/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.2388 - acc: 0.9733 - val_loss: 33.7419 - val_acc: 0.5215\n",
      "Epoch 75/100\n",
      "10592/10592 [==============================] - 2s 185us/step - loss: 0.2115 - acc: 0.9784 - val_loss: 36.4753 - val_acc: 0.5887\n",
      "Epoch 76/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 0.1982 - acc: 0.9779 - val_loss: 35.6457 - val_acc: 0.5661\n",
      "Epoch 77/100\n",
      "10592/10592 [==============================] - 2s 186us/step - loss: 0.2024 - acc: 0.9774 - val_loss: 38.9447 - val_acc: 0.5921\n",
      "Epoch 78/100\n",
      "10592/10592 [==============================] - 2s 185us/step - loss: 0.2373 - acc: 0.9757 - val_loss: 35.6552 - val_acc: 0.5366\n",
      "Epoch 79/100\n",
      "10592/10592 [==============================] - 2s 187us/step - loss: 0.2202 - acc: 0.9754 - val_loss: 39.8547 - val_acc: 0.6110\n",
      "Epoch 80/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 0.2149 - acc: 0.9769 - val_loss: 37.2195 - val_acc: 0.5600\n",
      "Epoch 81/100\n",
      "10592/10592 [==============================] - 2s 185us/step - loss: 0.2032 - acc: 0.9794 - val_loss: 38.0622 - val_acc: 0.5804\n",
      "Epoch 82/100\n",
      "10592/10592 [==============================] - 2s 187us/step - loss: 0.2210 - acc: 0.9797 - val_loss: 36.8710 - val_acc: 0.5544\n",
      "Epoch 83/100\n",
      "10592/10592 [==============================] - 2s 187us/step - loss: 0.2010 - acc: 0.9806 - val_loss: 38.7169 - val_acc: 0.6031\n",
      "Epoch 84/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.1967 - acc: 0.9808 - val_loss: 35.9830 - val_acc: 0.5295\n",
      "Epoch 85/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.2109 - acc: 0.9767 - val_loss: 36.0009 - val_acc: 0.5457\n",
      "Epoch 86/100\n",
      "10592/10592 [==============================] - 2s 197us/step - loss: 0.2018 - acc: 0.9791 - val_loss: 36.0642 - val_acc: 0.4841\n",
      "Epoch 87/100\n",
      "10592/10592 [==============================] - 2s 186us/step - loss: 0.1910 - acc: 0.9794 - val_loss: 36.8077 - val_acc: 0.5378\n",
      "Epoch 88/100\n",
      "10592/10592 [==============================] - 2s 191us/step - loss: 0.1723 - acc: 0.9826 - val_loss: 36.9126 - val_acc: 0.5389\n",
      "Epoch 89/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.1981 - acc: 0.9801 - val_loss: 36.9576 - val_acc: 0.5517\n",
      "Epoch 90/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.1935 - acc: 0.9779 - val_loss: 35.8902 - val_acc: 0.5136\n",
      "Epoch 91/100\n",
      "10592/10592 [==============================] - 2s 188us/step - loss: 0.1799 - acc: 0.9836 - val_loss: 41.8719 - val_acc: 0.5748\n",
      "Epoch 92/100\n",
      "10592/10592 [==============================] - 2s 185us/step - loss: 0.1897 - acc: 0.9826 - val_loss: 39.7757 - val_acc: 0.5861\n",
      "Epoch 93/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.1772 - acc: 0.9824 - val_loss: 41.4727 - val_acc: 0.5880\n",
      "Epoch 94/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.2033 - acc: 0.9798 - val_loss: 36.0394 - val_acc: 0.5151\n",
      "Epoch 95/100\n",
      "10592/10592 [==============================] - 2s 196us/step - loss: 0.1972 - acc: 0.9818 - val_loss: 41.3713 - val_acc: 0.5627\n",
      "Epoch 96/100\n",
      "10592/10592 [==============================] - 2s 192us/step - loss: 0.1754 - acc: 0.9823 - val_loss: 39.0665 - val_acc: 0.5574\n",
      "Epoch 97/100\n",
      "10592/10592 [==============================] - 2s 202us/step - loss: 0.1942 - acc: 0.9805 - val_loss: 41.0931 - val_acc: 0.5419\n",
      "Epoch 98/100\n",
      "10592/10592 [==============================] - 2s 189us/step - loss: 0.1821 - acc: 0.9818 - val_loss: 39.5821 - val_acc: 0.5759\n",
      "Epoch 99/100\n",
      "10592/10592 [==============================] - 2s 190us/step - loss: 0.1983 - acc: 0.9809 - val_loss: 43.3976 - val_acc: 0.5997\n",
      "Epoch 100/100\n",
      "10592/10592 [==============================] - 2s 192us/step - loss: 0.1816 - acc: 0.9826 - val_loss: 41.2562 - val_acc: 0.5793\n"
     ]
    }
   ],
   "source": [
    "def Embedding_pooling_model_multiclass():\n",
    "\n",
    "  cb = EarlyStopping(monitor='val_acc', mode='auto', patience=15)\n",
    "  model=keras.Sequential()\n",
    "  model.add(keras.layers.Embedding(VOCAB_SIZE, 100, input_length=EMBED_SIZE))\n",
    "  model.add(GlobalAveragePooling1DMasked())\n",
    "  model.add(keras.layers.Dense(16, activation='relu'))\n",
    "  model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "  model.summary()\n",
    "  model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "embedding_model_multiclass = Embedding_pooling_model_multiclass()\n",
    "class_weight = {0: 6.2,\n",
    "                1: 22.97,\n",
    "                2: 7.95, \n",
    "                3: 17.7, \n",
    "                4: 1}\n",
    "\n",
    "history3 = embedding_model_multiclass.fit(Xtrain,Ytrain,epochs=100,\n",
    "                                          validation_split=0.2, batch_size=32, class_weight=class_weight,\n",
    "                                          verbose=1,workers = 3, use_multiprocessing=True)#, callbacks = [cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "tkmE0qdAs3ZV",
    "outputId": "b1a9c9d9-e83e-450c-fe40-bd514f6b8238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.09      0.11        35\n",
      "           1       0.30      0.31      0.30       100\n",
      "           2       0.16      0.12      0.13        78\n",
      "           3       0.19      0.26      0.22        27\n",
      "           4       0.79      0.82      0.80       620\n",
      "\n",
      "    accuracy                           0.65       860\n",
      "   macro avg       0.31      0.32      0.31       860\n",
      "weighted avg       0.63      0.65      0.64       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(classification_report(Ytest, np.argmax(embedding_model_multiclass.predict(Categorical_test), axis =1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "5RZpsROctlvo",
    "outputId": "a9a5216a-4a01-4f7b-f3a4-d83d7650fe0b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_weight = {0: 20,\n",
    "                1: 2,\n",
    "                2: 1.5, \n",
    "                3: 18, \n",
    "                4: 1}\n",
    "plotting_history(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKUmQpOqIRZq"
   },
   "source": [
    "### Additional model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8tQcGLnojqr"
   },
   "source": [
    "#### Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1YLpaWaToit-",
    "outputId": "57a3b5aa-7c0b-45d3-ec41-68dee7ab5a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 162, 100)          1821800   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 157, 3)            1803      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_mas (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 20        \n",
      "=================================================================\n",
      "Total params: 1,823,623\n",
      "Trainable params: 1,823,623\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "10592/10592 [==============================] - 3s 323us/step - loss: 1.2273 - acc: 0.4515 - val_loss: 1.0187 - val_acc: 0.6688\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 2s 207us/step - loss: 1.0120 - acc: 0.6674 - val_loss: 1.0062 - val_acc: 0.6688\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 2s 205us/step - loss: 0.9831 - acc: 0.6681 - val_loss: 0.9730 - val_acc: 0.6726\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 2s 210us/step - loss: 0.8849 - acc: 0.7069 - val_loss: 0.9330 - val_acc: 0.6873\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 2s 207us/step - loss: 0.7870 - acc: 0.7448 - val_loss: 0.9252 - val_acc: 0.6915\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 2s 205us/step - loss: 0.7116 - acc: 0.7701 - val_loss: 0.9271 - val_acc: 0.6892\n",
      "Epoch 7/20\n",
      "10592/10592 [==============================] - 2s 208us/step - loss: 0.6488 - acc: 0.7929 - val_loss: 0.9374 - val_acc: 0.6881\n",
      "Epoch 8/20\n",
      "10592/10592 [==============================] - 2s 206us/step - loss: 0.5938 - acc: 0.8117 - val_loss: 0.9571 - val_acc: 0.6869\n",
      "Epoch 9/20\n",
      "10592/10592 [==============================] - 2s 204us/step - loss: 0.5503 - acc: 0.8246 - val_loss: 0.9810 - val_acc: 0.6847\n",
      "Epoch 10/20\n",
      "10592/10592 [==============================] - 2s 205us/step - loss: 0.5085 - acc: 0.8374 - val_loss: 1.0123 - val_acc: 0.6832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        35\n",
      "           1       0.37      0.45      0.41       100\n",
      "           2       0.31      0.14      0.19        78\n",
      "           3       0.00      0.00      0.00        27\n",
      "           4       0.81      0.91      0.86       620\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.30      0.30      0.29       860\n",
      "weighted avg       0.65      0.72      0.68       860\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cb = EarlyStopping(monitor='val_acc', mode='auto', patience=5)\n",
    "def Conv_Model():\n",
    "  Num_filters =3\n",
    "  Filter_width=6\n",
    "  cb = EarlyStopping(monitor='val_acc', mode='auto', patience=5)\n",
    "  model=keras.Sequential()\n",
    "  model.add(keras.layers.Embedding(VOCAB_SIZE, 100, input_length=EMBED_SIZE))\n",
    "  model.add(keras.layers.Conv1D(Num_filters, kernel_size=Filter_width, activation='relu'))\n",
    "  model.add(GlobalAveragePooling1DMasked())\n",
    "  #model.add(keras.layers.Dropout(0.5))\n",
    "  model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "  return model\n",
    "conv_model = Conv_Model()\n",
    "conv_hist = conv_model.fit(Xtrain,Ytrain,epochs=20, validation_split=0.2, \n",
    "                           batch_size=32,verbose=1,workers = 3, \n",
    "                           use_multiprocessing=True, callbacks = [cb])\n",
    "print(classification_report(Ytest, np.argmax(conv_model.predict(Categorical_test), axis =1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjawbSOyooRT"
   },
   "source": [
    "#### Embedding-Pooling (increased_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OX8dFEehkCAq",
    "outputId": "f95f5bb7-5e2f-4d2d-b146-61fe08a41433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 162, 100)          1821800   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_mas (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,889,933\n",
      "Trainable params: 1,889,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "10592/10592 [==============================] - 3s 277us/step - loss: 1.0429 - acc: 0.6674 - val_loss: 1.0018 - val_acc: 0.6688\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 2s 221us/step - loss: 0.9663 - acc: 0.6689 - val_loss: 0.9412 - val_acc: 0.6711\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 2s 227us/step - loss: 0.8036 - acc: 0.7204 - val_loss: 0.9117 - val_acc: 0.6941\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 2s 226us/step - loss: 0.6786 - acc: 0.7632 - val_loss: 0.9699 - val_acc: 0.6877\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 2s 218us/step - loss: 0.6127 - acc: 0.7828 - val_loss: 1.0807 - val_acc: 0.6884\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 2s 219us/step - loss: 0.5651 - acc: 0.7973 - val_loss: 1.1755 - val_acc: 0.6344\n",
      "Epoch 7/20\n",
      "10592/10592 [==============================] - 2s 223us/step - loss: 0.5285 - acc: 0.8063 - val_loss: 1.2118 - val_acc: 0.6805\n",
      "Epoch 8/20\n",
      "10592/10592 [==============================] - 2s 217us/step - loss: 0.4562 - acc: 0.8343 - val_loss: 1.4844 - val_acc: 0.6983\n",
      "Epoch 9/20\n",
      "10592/10592 [==============================] - 2s 222us/step - loss: 0.3850 - acc: 0.8638 - val_loss: 1.4142 - val_acc: 0.6782\n",
      "Epoch 10/20\n",
      "10592/10592 [==============================] - 2s 219us/step - loss: 0.3305 - acc: 0.8846 - val_loss: 1.5289 - val_acc: 0.6518\n",
      "Epoch 11/20\n",
      "10592/10592 [==============================] - 2s 224us/step - loss: 0.3003 - acc: 0.8913 - val_loss: 1.5538 - val_acc: 0.6594\n",
      "Epoch 12/20\n",
      "10592/10592 [==============================] - 2s 227us/step - loss: 0.2677 - acc: 0.9024 - val_loss: 1.7746 - val_acc: 0.6745\n",
      "Epoch 13/20\n",
      "10592/10592 [==============================] - 2s 220us/step - loss: 0.2615 - acc: 0.9092 - val_loss: 1.8491 - val_acc: 0.6677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.09      0.12        35\n",
      "           1       0.32      0.44      0.37       100\n",
      "           2       0.39      0.15      0.22        78\n",
      "           3       0.00      0.00      0.00        27\n",
      "           4       0.81      0.88      0.84       620\n",
      "\n",
      "    accuracy                           0.70       860\n",
      "   macro avg       0.34      0.31      0.31       860\n",
      "weighted avg       0.67      0.70      0.68       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Embedding_pooling_model_multiclass_extra_layer():\n",
    "  cb = EarlyStopping(monitor='val_acc', mode='auto', patience=5)\n",
    "  model=keras.Sequential()\n",
    "  model.add(keras.layers.Embedding(VOCAB_SIZE, 100, input_length=EMBED_SIZE))\n",
    "  model.add(GlobalAveragePooling1DMasked())\n",
    "  model.add(keras.layers.Dense(256, activation='relu'))\n",
    "  model.add(keras.layers.Dense(128, activation='relu'))\n",
    "  model.add(keras.layers.Dense(64, activation='relu'))\n",
    "  model.add(keras.layers.Dense(16, activation='relu'))\n",
    "  model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "  model.summary()\n",
    "  model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "embedding_pooling_model_multiclass_extra_layer = Embedding_pooling_model_multiclass_extra_layer()\n",
    "history4 = embedding_pooling_model_multiclass_extra_layer.fit(Xtrain,Ytrain,epochs=20,\n",
    "                                                              validation_split=0.2, batch_size=32,\n",
    "                                                              verbose=1,workers = 3, use_multiprocessing=True, callbacks = [cb])\n",
    "print(classification_report(Ytest, np.argmax(embedding_pooling_model_multiclass_extra_layer.predict(Categorical_test), axis =1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FHILG1HsZ8I"
   },
   "source": [
    "#### Seperate models into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "I6Iod8-8sfpy",
    "outputId": "fd5e9cf7-de7c-424d-acbb-5290f5265774"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputA (InputLayer)             (None, 162)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputB (InputLayer)             (None, 162)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inputC (InputLayer)             (None, 162)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "modelA_hidden1 (Dense)          (None, 648)          105624      inputA[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "modelB_hidden1 (Dense)          (None, 648)          105624      inputB[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "modelC_hidden1 (Dense)          (None, 648)          105624      inputC[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "outputA (Dense)                 (None, 1)            649         modelA_hidden1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "outputB (Dense)                 (None, 3)            1947        modelB_hidden1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "outputC (Dense)                 (None, 4)            2596        modelC_hidden1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "IndividualConcat (Concatenate)  (None, 8)            0           outputA[0][0]                    \n",
      "                                                                 outputB[0][0]                    \n",
      "                                                                 outputC[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Final_Model_1 (Dense)           (None, 10)           90          IndividualConcat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Final_Model_2 (Dense)           (None, 128)          1408        Final_Model_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Final_Output (Dense)            (None, 5)            645         Final_Model_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 324,207\n",
      "Trainable params: 324,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "10592/10592 [==============================] - 4s 340us/step - loss: 1.0658 - acc: 0.6657 - val_loss: 1.0139 - val_acc: 0.6688\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 2s 180us/step - loss: 1.0184 - acc: 0.6674 - val_loss: 1.0135 - val_acc: 0.6688\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 2s 174us/step - loss: 1.0189 - acc: 0.6674 - val_loss: 1.0137 - val_acc: 0.6688\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 2s 176us/step - loss: 1.0182 - acc: 0.6674 - val_loss: 1.0136 - val_acc: 0.6688\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 2s 175us/step - loss: 1.0184 - acc: 0.6674 - val_loss: 1.0144 - val_acc: 0.6688\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 2s 177us/step - loss: 1.0190 - acc: 0.6674 - val_loss: 1.0136 - val_acc: 0.6688\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        35\n",
      "           1       0.00      0.00      0.00       100\n",
      "           2       0.00      0.00      0.00        78\n",
      "           3       0.00      0.00      0.00        27\n",
      "           4       0.72      1.00      0.84       620\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.14      0.20      0.17       860\n",
      "weighted avg       0.52      0.72      0.60       860\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "def pipeline_model():\n",
    "  inputA = keras.layers.Input((EMBED_SIZE,), dtype='float32', name='inputA')\n",
    "  modelA_hidden1 = keras.layers.Dense(648, activation='relu', name='modelA_hidden1')(inputA)\n",
    "  outputA = keras.layers.Dense(1, activation='sigmoid', name='outputA')(modelA_hidden1)\n",
    "\n",
    "  inputB = keras.layers.Input((EMBED_SIZE,), dtype='float32', name='inputB')\n",
    "  modelB_hidden1 = keras.layers.Dense(648, activation='relu', name='modelB_hidden1')(inputB)\n",
    "  outputB = keras.layers.Dense(3, activation='softmax', name='outputB')(modelB_hidden1)\n",
    "\n",
    "  inputC = keras.layers.Input((EMBED_SIZE,), dtype='float32', name='inputC')\n",
    "  modelC_hidden1 = keras.layers.Dense(648, activation='relu', name='modelC_hidden1')(inputC)\n",
    "  outputC = keras.layers.Dense(4, activation='softmax', name='outputC')(modelC_hidden1)\n",
    "\n",
    "  final_input = keras.layers.concatenate([outputA, outputB, outputC], name = 'IndividualConcat')\n",
    "  final_model_1 = keras.layers.Dense(10, activation='relu', name='Final_Model_1')(final_input)\n",
    "  final_model_2 = keras.layers.Dense(128, activation='relu', name='Final_Model_2')(final_model_1)\n",
    "  final_output = keras.layers.Dense(5, activation='softmax', name='Final_Output')(final_model_2)\n",
    "\n",
    "  model= keras.Model(inputs=[inputA, inputB, inputC], output=[final_output])\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  #plot_model(model, to_file='model.png')\n",
    "  model.summary()\n",
    "  return model\n",
    "pipeline_model = pipeline_model()\n",
    "pipeline_history = pipeline_model.fit([Xtrain, Xtrain, Xtrain],Ytrain,epochs=20, validation_split=0.2, \n",
    "                                      batch_size=32,verbose=1,workers = 3, \n",
    "                                      use_multiprocessing=True, callbacks = [cb])\n",
    "print(classification_report(Ytest, np.argmax(pipeline_model.predict([Categorical_test, Categorical_test, Categorical_test]), axis =1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wTTS0iYWvw9"
   },
   "source": [
    "#### Glove file embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cWZftDUaW7jv",
    "outputId": "13f21f14-5f52-4a76-89b2-56d393d096ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-29 21:44:12--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2020-02-29 21:44:12--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2020-02-29 21:44:13--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: glove.6B.zip\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  1.81MB/s    in 6m 31s  \n",
      "\n",
      "2020-02-29 21:50:44 (2.10 MB/s) - glove.6B.zip saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "import zipfile\n",
    "import os\n",
    "path = 'glove.6B.zip'\n",
    "directory = os.getcwd()\n",
    "zipfile_ = zipfile.ZipFile(path, 'r')\n",
    "zipfile_.extractall(directory)\n",
    "zipfile_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1bZerrZQTMgL"
   },
   "outputs": [],
   "source": [
    "def readGloveFile(gloveFile):\n",
    "    with open(gloveFile, 'r') as f:\n",
    "        wordToGlove = {}  \n",
    "        wordToIndex = {}  \n",
    "        indexToWord = {}  \n",
    "\n",
    "        for line in f:\n",
    "            record = line.strip().split()\n",
    "            token = record[0] \n",
    "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
    "            \n",
    "        tokens = sorted(wordToGlove.keys())\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            kerasIdx = idx + 1  \n",
    "            wordToIndex[tok] = kerasIdx \n",
    "            indexToWord[kerasIdx] = tok \n",
    "\n",
    "    return wordToIndex, indexToWord, wordToGlove\n",
    "  \n",
    "from keras.initializers import Constant\n",
    "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
    "    vocabLen = len(wordToIndex) + 1  \n",
    "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
    "   \n",
    "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
    "    for word, index in wordToIndex.items():\n",
    "        embeddingMatrix[index, :] = wordToGlove[word] \n",
    "\n",
    "    embeddingLayer = keras.layers.Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable)\n",
    "    return embeddingLayer\n",
    "\n",
    "wordToIndex, indexToWord, wordToGlove = readGloveFile('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "KEfCCjXBXDwY",
    "outputId": "8a3bbee5-460b-456d-f871-54cbdbcf4ab1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, None, 300)         120000300 \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_mas (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                4816      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 120,005,201\n",
      "Trainable params: 120,005,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def glove_model():\n",
    "  model_glove = keras.Sequential()\n",
    "  model_glove.model.add(createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable=True))\n",
    "  model_glove.add(GlobalAveragePooling1DMasked())\n",
    "  #model_glove.add(keras.layers.Dense(256, activation='relu'))###\n",
    "  #model_glove.add(keras.layers.Dense(128, activation='relu'))###\n",
    "  #model_glove.add(keras.layers.Dense(64, activation='relu'))###\n",
    "  model_glove.add(keras.layers.Dense(16, activation='relu'))\n",
    "  model_glove.add(keras.layers.Dense(5, activation='softmax'))\n",
    "  model_glove.summary()\n",
    "  model_glove.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "  return model_glove\n",
    "model_glove = glove_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QgRmCuQpXPju",
    "outputId": "18374d93-6cac-429b-f8ff-3bc4948eee89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, None, 300)         120000300 \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_mas (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 120,119,633\n",
      "Trainable params: 120,119,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:421: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 120000300 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "10592/10592 [==============================] - 20s 2ms/step - loss: 1.0377 - acc: 0.6649 - val_loss: 0.9870 - val_acc: 0.6688\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 14s 1ms/step - loss: 0.9435 - acc: 0.6701 - val_loss: 0.9214 - val_acc: 0.6915\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 14s 1ms/step - loss: 0.7594 - acc: 0.7346 - val_loss: 0.9010 - val_acc: 0.7081\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 14s 1ms/step - loss: 0.6038 - acc: 0.7902 - val_loss: 0.9880 - val_acc: 0.6869\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 14s 1ms/step - loss: 0.4918 - acc: 0.8319 - val_loss: 1.1504 - val_acc: 0.6813\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 14s 1ms/step - loss: 0.4147 - acc: 0.8532 - val_loss: 1.2179 - val_acc: 0.6677\n",
      "Epoch 7/20\n",
      "10592/10592 [==============================] - 14s 1ms/step - loss: 0.3460 - acc: 0.8799 - val_loss: 1.3866 - val_acc: 0.6292\n",
      "Epoch 8/20\n",
      "10592/10592 [==============================] - 14s 1ms/step - loss: 0.3141 - acc: 0.8913 - val_loss: 1.5492 - val_acc: 0.6647\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXjU5bn/8fdNBGNAdtxACCpFdogp\n6BGtiAtahaOlCgYVrSIetdZqe6zQoz8Vj8e2FrXWFqkLNUKtVKutK4oiVStBCQioICIGUAERWVS2\n+/fH8w2ZhEkygUxmJvm8rmuumfluc88E5p5nN3dHRESkokapDkBERNKTEoSIiMSlBCEiInEpQYiI\nSFxKECIiEpcShIiIxKUEIQkzsywz22RmHWvz2FQysyPMrNb7epvZSWa2POb5+2Z2XCLH7sFrTTaz\nG/b0fJHK7JPqACR5zGxTzNMc4FtgR/T8MncvrMn13H0H0Ky2j20I3L1rbVzHzC4BRrn7CTHXvqQ2\nri1SkRJEPebuu76go1+ol7j7jMqON7N93H17XcQmUh39e0w9VTE1YGZ2q5n9xcymmtlGYJSZHWNm\nb5rZl2a22szuNrPG0fH7mJmbWW70/JFo/7NmttHM3jCzzjU9Ntp/mpl9YGYbzOweM/uXmY2uJO5E\nYrzMzJaa2Xozuzvm3Cwz+62ZrTOzZcCQKj6fcWY2rcK2e83szujxJWa2OHo/H0a/7iu7VomZnRA9\nzjGzP0exLQSOqnDseDNbFl13oZkNjbb3An4HHBdV362N+Wxvijl/bPTe15nZk2Z2cCKfTU0+59J4\nzGyGmX1hZp+a2c9jXueX0WfylZkVmdkh8arzzGx26d85+jxnRa/zBTDezLqY2czoNdZGn1uLmPM7\nRe9xTbT/LjPLjmLuFnPcwWa2xczaVPZ+JQ53160B3IDlwEkVtt0KbAXOJPxY2A/4LjCAULo8DPgA\nuDI6fh/Agdzo+SPAWiAfaAz8BXhkD449ANgIDIv2/RTYBoyu5L0kEuPfgRZALvBF6XsHrgQWAh2A\nNsCs8N8g7uscBmwCmsZc+3MgP3p+ZnSMAScCXwO9o30nActjrlUCnBA9/jXwCtAK6AQsqnDsOcDB\n0d/kvCiGA6N9lwCvVIjzEeCm6PEpUYx9gWzg98DLiXw2NfycWwCfAVcD+wLNgf7Rvl8AxUCX6D30\nBVoDR1T8rIHZpX/n6L1tBy4Hsgj/Hr8DDAaaRP9O/gX8Oub9vBt9nk2j44+N9k0CJsS8zrXAE6n+\nf5hpt5QHoFsd/aErTxAvV3PedcBfo8fxvvT/EHPsUODdPTj2YuC1mH0GrKaSBJFgjEfH7P8bcF30\neBahqq103+kVv7QqXPtN4Lzo8WnA+1Uc+w/giuhxVQliRezfAviv2GPjXPdd4PvR4+oSxMPAbTH7\nmhPanTpU99nU8HM+H5hTyXEflsZbYXsiCWJZNTEML31d4DjgUyArznHHAh8BFj2fB5xd2/+v6vtN\nVUzySewTMzvSzP4ZVRl8BdwMtK3i/E9jHm+h6obpyo49JDYOD/+jSyq7SIIxJvRawMdVxAvwKDAy\nenxe9Lw0jjPM7N9R9ceXhF/vVX1WpQ6uKgYzG21mxVE1yZfAkQleF8L723U9d/8KWA+0jzkmob9Z\nNZ/zoYREEE9V+6pT8d/jQWb2mJmtjGJ4qEIMyz10iCjH3f9FKI0MNLOeQEfgn3sYU4OlBCEVu3j+\nkfCL9Qh3bw78D+EXfTKtJvzCBcDMjPJfaBXtTYyrCV8sparrhvsYcJKZtSdUgT0axbgf8Djwv4Tq\nn5bACwnG8WllMZjZYcB9hGqWNtF134u5bnVdclcRqq1Kr7c/oSprZQJxVVTV5/wJcHgl51W2b3MU\nU07MtoMqHFPx/f0fofddryiG0RVi6GRmWZXEMQUYRSjtPObu31ZynFRCCUIq2h/YAGyOGvkuq4PX\n/AeQZ2Znmtk+hHrtdkmK8THgJ2bWPmqw/O+qDnb3TwnVIA8RqpeWRLv2JdSLrwF2mNkZhLryRGO4\nwcxaWhgncmXMvmaEL8k1hFx5KaEEUeozoENsY3EFU4EfmVlvM9uXkMBec/dKS2RVqOpzfgroaGZX\nmtm+ZtbczPpH+yYDt5rZ4Rb0NbPWhMT4KaEzRJaZjSEmmVURw2Zgg5kdSqjmKvUGsA64zULD/35m\ndmzM/j8TqqTOIyQLqSElCKnoWuBCQqPxHwmNyUnl7p8B5wJ3Ev7DHw68Q/jlWNsx3ge8BCwA5hBK\nAdV5lNCmsKt6yd2/BK4BniA09A4nJLpE3EgoySwHniXmy8vd5wP3AG9Fx3QF/h1z7ovAEuAzM4ut\nKio9/zlCVdAT0fkdgYIE46qo0s/Z3TcAJwM/ICStD4DvRbt/BTxJ+Jy/IjQYZ0dVh5cCNxA6LBxR\n4b3FcyPQn5CongKmx8SwHTgD6EYoTawg/B1K9y8n/J2/dffXa/jehbIGHJG0EVUZrAKGu/trqY5H\nMpeZTSE0fN+U6lgykQbKSVowsyGEHkNfE7pJbiP8ihbZI1F7zjCgV6pjyVSqYpJ0MRBYRqh7PxU4\nS42KsqfM7H8JYzFuc/cVqY4nU6mKSURE4lIJQkRE4qo3bRBt27b13NzcVIchIpJR5s6du9bd43Yr\nT2qCiBoe7yLMqzLZ3W+vsL8T8AChz/sXhGmMS6J9FwLjo0NvdfeHq3qt3NxcioqKavkdiIjUb2ZW\n6WwCSatiiroq3kuYv6Y7MNLMulc47NfAFHfvTei7/b/Rua0J/Z8HEPpA32hmrZIVq4iI7C6ZbRD9\ngaXuvszdtwLTCF3OYnUHXo4ez4zZfyrwort/4e7rCYODKp2WWUREal8yE0R7yk+8VcLu8+sUA2dH\nj88C9o+mP0jkXMxsTDTXfNGaNWtqLXAREUl9I/V1wO+iBUNmESYU221mxsq4+yTCMH7y8/N366+7\nbds2SkpK+Oabb2onWkmK7OxsOnToQOPGlU0vJCKpkMwEsZLyM1Z2oMKMku6+iqgEYWbNgB+4+5dm\nthI4ocK5r9Q0gJKSEvbff39yc3MJE4RKunF31q1bR0lJCZ07d67+BBGpM8msYpoDdDGzzmbWBBhB\nmGxrFzNra2alMfyC0KMJ4HngFDNrFTVOnxJtq5FvvvmGNm3aKDmkMTOjTZs2KuWJ7IHCQsjNhUaN\nwn1hYe1eP2kJIppp8UrCF/tiwnzsC83sZovW2CWUEt43sw+AA4EJ0blfALcQkswc4OZoW40pOaQ/\n/Y1Eaq6wEMaMgY8/BvdwP2ZM7SaJejPVRn5+vlccB7F48WK6detWyRmSTvS3EqmZ3NyQFCrq1AmW\nL0/8OmY2193z4+3TVBtJtG7dOvr27Uvfvn056KCDaN++/a7nW7duTegaF110Ee+//36Vx9x7770U\n1nbZUkTS2opKpiCsbPueSHUvprRSWAjjxoUPuGNHmDABCvZ0qRWgTZs2zJs3D4CbbrqJZs2acd11\n15U7Ztfi4I3i5+oHH3yw2te54oor9jxIEclIHTvGL0F0rG4R3RpQCSJSF/V5pZYuXUr37t0pKCig\nR48erF69mjFjxpCfn0+PHj24+eabdx07cOBA5s2bx/bt22nZsiXXX389ffr04ZhjjuHzzz8HYPz4\n8UycOHHX8ddffz39+/ena9euvP56WEhr8+bN/OAHP6B79+4MHz6c/Pz8Xckr1o033sh3v/tdevbs\nydixYymtgvzggw848cQT6dOnD3l5eSyPyrC33XYbvXr1ok+fPowbN672PywRiWvCBMjJKb8tJyds\nry1KEJFx42DLlvLbtmwJ25Phvffe45prrmHRokW0b9+e22+/naKiIoqLi3nxxRdZtGjRbuds2LCB\n733vexQXF3PMMcfwwAMPxLlyKJW89dZb/OpXv9qVbO655x4OOuggFi1axC9/+UveeeeduOdeffXV\nzJkzhwULFrBhwwaee+45AEaOHMk111xDcXExr7/+OgcccABPP/00zz77LG+99RbFxcVce+21tfTp\niEh1Cgpg0qTQ5mAW7idN2rtaj4qUICJ1UZ8X6/DDDyc/v6xdaOrUqeTl5ZGXl8fixYvjJoj99tuP\n0047DYCjjjpq16/4is4+++zdjpk9ezYjRowAoE+fPvTo0SPuuS+99BL9+/enT58+vPrqqyxcuJD1\n69ezdu1azjzzTCAMbMvJyWHGjBlcfPHF7LfffgC0bt265h+EiOyxgoLQIL1zZ7ivzeQAaoPYpS7q\n82I1bdp01+MlS5Zw11138dZbb9GyZUtGjRoVd1xAkyZNdj3Oyspi+/btca+97777VntMPFu2bOHK\nK6/k7bffpn379owfP17jE0QaMJUgInVRn1eZr776iv3335/mzZuzevVqnn++xmMCq3Xsscfy2GOP\nAbBgwYK4JZSvv/6aRo0a0bZtWzZu3Mj06dMBaNWqFe3atePpp58GwgDELVu2cPLJJ/PAAw/w9ddf\nA/DFF3s0VEUkrSR78FkmUQkiUlo0q81eTInKy8uje/fuHHnkkXTq1Iljjz221l/jqquu4oILLqB7\n9+67bi1atCh3TJs2bbjwwgvp3r07Bx98MAMGDNi1r7CwkMsuu4xx48bRpEkTpk+fzhlnnEFxcTH5\n+fk0btyYM888k1tuuaXWYxepK6WdVUrbI0s7q0DdfBekGw2UayC2b9/O9u3byc7OZsmSJZxyyiks\nWbKEffZJj98I+ltJOqitwWeZpKqBcunx7SBJt2nTJgYPHsz27dtxd/74xz+mTXIQSRd13Vkl3ekb\nooFo2bIlc+fOTXUYImmtrjurpDs1UouIRFLZWSUdKUGIiETqYvBZJlEVk4hIjIKChpsQKlIJQkRE\n4lKCSKJBgwbtNuht4sSJXH755VWe16xZMwBWrVrF8OHD4x5zwgknULFbb0UTJ05kS8wEU6effjpf\nfvllIqGLiChBJNPIkSOZNm1auW3Tpk1j5MiRCZ1/yCGH8Pjjj+/x61dMEM888wwtW7bc4+uJSMOi\nBJFEw4cP55///OeuxYGWL1/OqlWrOO6443aNS8jLy6NXr178/e9/3+385cuX07NnTyBMgzFixAi6\ndevGWWedtWt6C4DLL79811ThN954IwB33303q1atYtCgQQwaNAiA3Nxc1q5dC8Cdd95Jz5496dmz\n566pwpcvX063bt249NJL6dGjB6ecckq51yn19NNPM2DAAPr168dJJ53EZ599BoSxFhdddBG9evWi\nd+/eu6bqeO6558jLy6NPnz4MHjy4Vj5bEUm+BtNI/ZOfQJzlD/ZK374QfbfG1bp1a/r378+zzz7L\nsGHDmDZtGueccw5mRnZ2Nk888QTNmzdn7dq1HH300QwdOrTS9Znvu+8+cnJyWLx4MfPnzycvL2/X\nvgkTJtC6dWt27NjB4MGDmT9/Pj/+8Y+58847mTlzJm3bti13rblz5/Lggw/y73//G3dnwIABfO97\n36NVq1YsWbKEqVOncv/993POOecwffp0Ro0aVe78gQMH8uabb2JmTJ48mTvuuIPf/OY33HLLLbRo\n0YIFCxYAsH79etasWcOll17KrFmz6Ny5s+ZrEskgKkEkWWw1U2z1krtzww030Lt3b0466SRWrly5\n65d4PLNmzdr1Rd27d2969+69a99jjz1GXl4e/fr1Y+HChXEn4os1e/ZszjrrLJo2bUqzZs04++yz\nee211wDo3Lkzffv2BSqfUrykpIRTTz2VXr168atf/YqFCxcCMGPGjHKr27Vq1Yo333yT448/ns6d\nOwOaElwkkzSYEkRVv/STadiwYVxzzTW8/fbbbNmyhaOOOgoIk9+tWbOGuXPn0rhxY3Jzc/doau2P\nPvqIX//618yZM4dWrVoxevTovZqiu3SqcAjThcerYrrqqqv46U9/ytChQ3nllVe46aab9vj1RCR9\nqQSRZM2aNWPQoEFcfPHF5RqnN2zYwAEHHEDjxo2ZOXMmH8cb3x/j+OOP59FHHwXg3XffZf78+UCY\nKrxp06a0aNGCzz77jGeffXbXOfvvvz8bN27c7VrHHXccTz75JFu2bGHz5s088cQTHHfccQm/pw0b\nNtC+fXsAHn744V3bTz75ZO69995dz9evX8/RRx/NrFmz+OijjwBNCS6SSZQg6sDIkSMpLi4ulyAK\nCgooKiqiV69eTJkyhSOPPLLKa1x++eVs2rSJbt268T//8z+7SiJ9+vShX79+HHnkkZx33nnlpgof\nM2YMQ4YM2dVIXSovL4/Ro0fTv39/BgwYwCWXXEK/fv0Sfj833XQTP/zhDznqqKPKtW+MHz+e9evX\n07NnT/r06cPMmTNp164dkyZN4uyzz6ZPnz6ce+65Cb+OiKSWpvuWtKC/lUhqVDXdt0oQIiISlxKE\niIjEVe8TRH2pQqvP9Deq37TGc+aq1wkiOzubdevW6Qsojbk769atIzs7O9WhSBKUrvH88cfgXrbG\ns5JEZkhqI7WZDQHuArKAye5+e4X9HYGHgZbRMde7+zNmlgssBt6PDn3T3cdW9VrxGqm3bdtGSUnJ\nXo0LkOTLzs6mQ4cONG7cONWhSC1riGs8Z5qUrEltZlnAvcDJQAkwx8yecvfYYb7jgcfc/T4z6w48\nA+RG+z509757E0Pjxo13jeAVkbqnNZ4zWzKrmPoDS919mbtvBaYBwyoc40Dz6HELYFUS4xGROlbZ\nWs4NdY3nTJPMBNEe+CTmeUm0LdZNwCgzKyGUHq6K2dfZzN4xs1fNLO4wXzMbY2ZFZla0Zs2aWgxd\nRGqD1njObKlupB4JPOTuHYDTgT+bWSNgNdDR3fsBPwUeNbPmFU9290nunu/u+e3atavTwEWkelrj\nObMlc7K+lcChMc87RNti/QgYAuDub5hZNtDW3T8Hvo22zzWzD4HvAFUvoSYiaUdrPGeuZJYg5gBd\nzKyzmTUBRgBPVThmBTAYwMy6AdnAGjNrFzVyY2aHAV2AZUmMVUREKkhaCcLdt5vZlcDzhC6sD7j7\nQjO7GShy96eAa4H7zewaQoP1aHd3MzseuNnMtgE7gbHurmlARUTqUL2erE9ERKqmyfpERKTGlCBE\nRCQuJQgREYlLCUJEROJSghARkbiUIEQykNZYkLqQzJHUIpIEpWssbNkSnpeusQAasSy1SyUIkQwz\nblxZcii1ZUvYLlKblCBEMozWWJC6ogQhkmG0xoLUFSUIkQyjNRakrihBiGQYrbEgdUW9mEQykNZY\nkLqgEoSIiMSlBCEiInEpQYiISFxKECIiEpcShIiIxKUEISIicSlBiIhIXEoQIiISlxKEiIjEpQQh\nIiJxKUGIiEhcShAiIhKXEoSIiMSlBCEiInEpQYiISFxKECIiEldSE4SZDTGz981sqZldH2d/RzOb\naWbvmNl8Mzs9Zt8vovPeN7NTkxmniIjsLmkryplZFnAvcDJQAswxs6fcfVHMYeOBx9z9PjPrDjwD\n5EaPRwA9gEOAGWb2HXffkax4RUSkvGSWIPoDS919mbtvBaYBwyoc40Dz6HELYFX0eBgwzd2/dfeP\ngKXR9UREpI4kM0G0Bz6JeV4SbYt1EzDKzEoIpYeranAuZjbGzIrMrGjNmjW1FbeIiJD6RuqRwEPu\n3gE4HfizmSUck7tPcvd8d89v165d0oIUEWmIktYGAawEDo153iHaFutHwBAAd3/DzLKBtgmeKyIi\nSZTMEsQcoIuZdTazJoRG56cqHLMCGAxgZt2AbGBNdNwIM9vXzDoDXYC3khiriIhUkLQE4e7bgSuB\n54HFhN5KC83sZjMbGh12LXCpmRUDU4HRHiwEHgMWAc8BV6gHkyRTYSHk5kKjRuG+sDDVEYmknrl7\nqmOoFfn5+V5UVJTqMCQDFRbCmDGwZUvZtpwcmDQJCgpSF5dIXTCzue6eH29fqhupRVJu3LjyyQHC\n83HjUhOPSLpQgpAGb8WKmm0XaSiUIKTB69ixZttFGgolCGnwJkwIbQ6xcnLCdpGGrNoEYWZXmVmr\nughGJBUKCkKDdKdOYBbu1UAtkthAuQMJE+29DTwAPO/1peuTSKSgQAlBpKJqSxDuPp4wUO1PwGhg\niZndZmaHJzk2ERFJoYTaIKISw6fRbTvQCnjczO5IYmwiIpJC1VYxmdnVwAXAWmAy8DN33xZNqrcE\n+HlyQxQRkVRIpA2iNXC2u38cu9Hdd5rZGckJS0REUi2RKqZngS9Kn5hZczMbAODui5MVmIiIpFYi\nCeI+YFPM803RNhERqccSSRAW263V3XeS3HUkREQkDSSSIJaZ2Y/NrHF0uxpYluzAREQktRJJEGOB\n/yCs6FYCDADGJDMoERFJvWqritz9c8JqcCIi0oAkMg4im7B2dA/CkqAAuPvFSYxLRERSLJEqpj8D\nBwGnAq8CHYCNyQxKRERSL5EEcYS7/xLY7O4PA98ntEOIiEg9lkiC2Bbdf2lmPYEWwAHJC0lERNJB\nIuMZJkXrQYwHngKaAb9MalQiIpJyVSaIaEK+r9x9PTALOKxOohIRkZSrsoopGjWt2VpFRBqgRNog\nZpjZdWZ2qJm1Lr0lPTIREUmpRNogzo3ur4jZ5qi6SUSkXktkJHXnughERETSSyIjqS+It93dp9R+\nOCIiki4SqWL6bszjbGAw8DagBCEiUo8lUsV0VexzM2sJTEtaRCIikhYS6cVU0WYgoXYJMxtiZu+b\n2VIzuz7O/t+a2bzo9oGZfRmzb0fMvqf2IE4REdkLibRBPE3otQQhoXQHHkvgvCzgXuBkwjoSc8zs\nKXdfVHqMu18Tc/xVQL+YS3zt7n0TeRMiIlL7EmmD+HXM4+3Ax+5eksB5/YGl7r4MwMymAcOARZUc\nPxK4MYHriohIHUgkQawAVrv7NwBmtp+Z5br78mrOaw98EvO8dDW63ZhZJ0K11csxm7PNrIiQlG53\n9yfjnDeGaHW7jh07JvBWREQkUYm0QfwV2BnzfEe0rTaNAB539x0x2zq5ez5wHjDRzA6veJK7T3L3\nfHfPb9euXS2HJCLSsCWSIPZx962lT6LHTRI4byVwaMzzDtG2eEYAU2M3uPvK6H4Z8Arl2ydERCTJ\nEkkQa8xsaOkTMxsGrE3gvDlAFzPrbGZNCElgt95IZnYk0Ap4I2ZbKzPbN3rcFjiWytsuREQkCRJp\ngxgLFJrZ76LnJUDc0dWx3H27mV0JPA9kAQ+4+0IzuxkocvfSZDECmObuHnN6N+CPZraTkMRuj+39\nJCIiyWflv5erONCsGYC7b0pqRHsoPz/fi4qKUh2GiEhGMbO5UXvvbqqtYjKz28yspbtvcvdNUfXP\nrbUfpoiIpJNE2iBOc/ddI5yj1eVOT15IIiKSDhJJEFmlDcYQxkEA+1ZxvAgAhYWQmwuNGoX7wsJU\nRyQiNZFII3Uh8JKZPQgYMBp4OJlBSeYrLIQxY2DLlvD844/Dc4CCgtTFJSKJq7YE4e7/B9xK6FnU\nldArqVOS45IMN25cWXIotWVL2C4imSHR2Vw/I0zY90PgRGBx0iKSemHFipptF5H0U2kVk5l9hzCB\n3kjCwLi/ELrFDqqj2CSDdewYqpXibReRzFBVCeI9QmnhDHcf6O73EOZhEqnWhAmQk1N+W05O2C4i\nmaGqBHE2sBqYaWb3m9lgQiO1SLUKCmDSJOjUCczC/aRJaqAWySTVjqQ2s6aEdRxGEkoUU4An3P2F\n5IeXOI2kFhGpub0aSe3um939UXc/kzAj6zvAf9dyjCIikmZqtCa1u6+P1mAYnKyAREQkPdQoQYiI\nSMOhBCEiInEpQYiISFxKECIiEpcShIiIxKUEISIicSlBiIhIXEoQIiISlxKEiIjEpQQhIiJxKUGI\niEhcShAiIhKXEoQkzfr18Nxz8NlnqY5ERPZEpUuOitSUOxQXw7PPwjPPwBtvwI4dkJUFp54KF1wA\nQ4fCfvulOlIRSYQShOyVr76CGTNCQnj2WVi1KmzPy4Nf/AIGDoRXX4U//xlGjIDmzeGcc0KyOPZY\naKQyrEjaqnZFuUyhFeXqhjssWhQSwjPPwOzZsH07tGgBp5wCp58OQ4bAQQeVP2/nTnjlFZgyBR5/\nHDZvhtxcOP/8cOvSJRXvRkSqWlEuqQnCzIYAdwFZwGR3v73C/t8Cg6KnOcAB7t4y2nchMD7ad6u7\nP1zVaylBJM+mTfDyy2WlhBUrwvbevUNCOP10OPpoaNw4sett3gxPPBGSxYwZIekcc0woVZxzDrRu\nnbz3IiLlpSRBmFkW8AFwMlACzAFGuvuiSo6/Cujn7hebWWugCMgHHJgLHOXu6yt7PSWI2uMOH3xQ\nlhBefRW2boVmzeDkk+G008KtQ4e9f62VK+HRR+Hhh2HhQmjSBM48MySLIUPCcxFJnqoSRDLbIPoD\nS919WRTENGAYEDdBACOBG6PHpwIvuvsX0bkvAkOAqUmMt0HbsiVUAZU2MC9bFrZ36wZXXRVKCQMH\n1v4Xdvv28LOfwXXXwbx5oVTx6KMwfTq0bRvaLS64APLzwax2X1tEqpbMBNEe+CTmeQkwIN6BZtYJ\n6Ay8XMW57ZMQY4P24YdlCWHmTPjmG8jJgRNPDF/Yp50W2gnqghn06xdud9wBL7wQksX998PvfgdH\nHhkSxahRcOihdROTSEOXLr2YRgCPu/uOmpxkZmOAMQAdO3ZMRlz1yrffwqxZZQ3MH3wQtnfpApdd\nFkoJxx8P2dmpjbNxY/j+98Ptyy9Do/aUKXDDDTBuHAwaFJLF2WfD/vunNlaR+iyZnQxXArG/9TpE\n2+IZQfnqo4TOdfdJ7p7v7vnt2rXby3Drp48/hj/8IYw/aN069DS67z447DC4+25YsiQkiokTw75U\nJ4eKWraESy4Jie3DD+Gmm8J7Gj069JQ6//xQ2thRo58WIpKIZDZS70NopB5M+HKfA5zn7gsrHHck\n8BzQ2aNgokbquUBedNjbhEbqLyp7PTVSB1u3wr/+VVZKWBS1+OTmlvU4GjQoVCVlKvcwCG/KFPjL\nX0Ip45BDoKAglCx69kx1hCKZI5XdXE8HJhK6uT7g7hPM7GagyN2fio65Cch29+srnHsxcEP0dIK7\nP1jVazXkBLFyZVlbwowZsHFjqKY5/viypNC1a/1s5P3mG/jHP8JAvGeeCWMy+vULiWLkSDjwwFRH\nKJLeUpYg6lJDShDbt4df0H5ck3oAABC4SURBVKXdUIuLw/YOHcoSwoknNrz6+TVrYNq0ULIoKgpT\nfAwZEpLFmWdqig+ReJQg6oHSapXJk8Mgsy+/DF+AAweGhHDaaaFqpT6WEvbEokWhVPHII1BSEkZ6\n//CHIVkMHKjPSaSUEkQGW7cu/CKePDl86TVtCsOHwxlnwEknhUZcqdyOHWVTfEyfHkZxd+5cNsXH\nEUekOkKR1FKCyDA7d4ZxCZMnw9/+FhqeBwwIvXnOPbfhVR3VlnhTfPzHf5RN8dGqVaojFKl7ShAZ\nYvVqePBB+NOfwkjmVq3Cr9wf/SjMeyS1p6SkbIqPRYvKpvg444zQlnPwweHWqpWqo6R+U4JIY9u3\nh0V1Jk8OvXF27IATTgilhbPPVsNqsrnDO++UTfGxZk35/fvuG8ZbHHJIWdKIvZVub9tWU5dLZlKC\nSEPLl4eSwoMPhm6qBx4YBn/96Eea+jpVtm0Lf5fVq3e/rVpV9nh9nCkj99kn/A0rSyCltwMPDMeK\npItUTdYnFWzdCn//eygtvPhi2DZkCNxzT6jaSHS6bEmOxo1Dcq4uQX/zDXz6afmkEXtbsQLefHP3\n0giE6qp27eInj9ikctBBofQikkpKEHXgvfdCUnj4YVi7Nkw2d+ONcNFFoCmkMk92dhiZXt1Ehtu2\nhfW4KyuJrF4dxrB89ln8qUJat666RNK+fYhBVVuSLEoQSbJlS5hkbvJkeO21UK0wdChcemlYUyEr\nK9URSrI1bhwavKtbN2PHjlDaqKpq64MPwv22beXPbdUqLN163HFhfMdRR6nkIbVHCaKWzZsXpqgu\nLIQNG0J1xf/9H1x4oaZ9kPiyskKV0kEHhWlCKuMOX3xRljhKq7Jmzw4dHCCUbvr3D8li4MDQjbdF\ni7p5H1L/qJG6Fnz1FUydGkoLRUXhF9zw4aG0cPzx6iYpyff552GSxtmzQ4n17bdDycQsdJEeOLCs\nlNFeK6tIDPViSgL38Ovt/vvDjKJbtkCvXiEpjBqlQVeSWps3w7//HZLF7NlhmpbNm8O+zp3LShjH\nHRcWY9KPmIZLCaIWrVsX5viZPDmsody0aZg19NJL4bvf1X80SU/bt4fqz9ISxuzZodQB0KZN+XaM\nvDytBd6QKEHspZ07w3w+99+vqS+kfnCHpUvLksXs2WHxKAiDMwcMKCtlHHMMNG+e2ngleZQg9tDq\n1fDQQ2FA24cfhonxzj8/JIZUTH1RWBiW3FyxInSPnTAhLJIjUhs+/bR8O8Y774QfR40aQZ8+5dsx\nDj441dFKbVGCqIHt2+H550NpIZ2mvigshDFjQltHqZwcmDRJSUKSY+PGsl5Ss2eHx6X//g47rCxZ\nDBxYfxekagiUIBKwfDk88EC4rVwJBxwQpr645JL0mPoiNzesxVxRp04hdpFk27YtlCpi2zHWrg37\n2rYt3/Ddr59mBsgUShBV+OSTkARip7645JIws2c6/QNv1CjUG1dkFqoBROqaexjAF9uO8eGHYV9O\nDhx9dPl2jGbNUhuvxKe5mKowY0ZogHYP/cMLCkJVUrrp2DF+CUJTdUiqmIWqpa5dw48qCAP4Ytsx\nbr01/IDJyoJTToHf/776KUokfTToWVwKC+HKK0OvJAhVS2PGhO3pZsKE8KssVk5O2C6SLg45JCzt\netddYbDe+vWhTe/nPw8Jo2dPuO8+lXozRYNOEOPGlW/0hfB83LjUxFOVgoLQIN2pU/jl1qmTGqgl\n/TVvHkoOt90G774bqpr+67/CtnglYqmZnTvhj3+EO+5IzvUbdIJYsaJm21OtoCA0SO/cGe6VHCST\ndOoEL7wAf/hDGOXds2f4cqsnzaB1bt68MNfW2LFhieJklMoadIKorP5e9foiyWEGl10GCxaEwXhj\nx4bZjVWaSNzGjXDNNWHm3o8+gkcegWeeSc607w06QaheXyQ1cnNDz8H77lNpIlHuYQmBbt1CG8+Y\nMWGtmYKC5I1BadAJQvX6IqljFkoQCxaEKcrHjoVTT03fKt5UWrYMvv/90AGgXbsw+eJ99yV/UtAG\nnSBA9foiqVZamvj97+H110Np4v77VZoA+PbbUKPRo0foBfbb38KcOaF6ri40+AQhIqnXqBFcfnko\nTeTnh+qTIUMadmnilVegb18YPz6sWf/ee/CTn4TVKeuKEoSIpI3OncPg1XvvDQPuGmJp4vPP4YIL\nYNCgUIJ45hn4619Ts9CTEoSIpJVGjcJYiQULQk+dMWPgtNPCtDj1WemYhq5dYdq0UHJYuDC891RJ\naoIwsyFm9r6ZLTWz6ys55hwzW2RmC83s0ZjtO8xsXnR7Kplxikj66dwZXnoJfve7MHVHz55h6v36\nWJqYNy8s2jR2bKhWmj8fbrkldbNHl0pagjCzLOBe4DSgOzDSzLpXOKYL8AvgWHfvAfwkZvfX7t43\nug1NVpwikr4aNYIrrghfmHl5Yc6n00+vP6WJjRvhpz8NJaUPPwyrVb78clgGNh0kswTRH1jq7svc\nfSswDRhW4ZhLgXvdfT2Au3+exHhEJEMddlgoTdxzD8yalfmlCXeYPj2MaZg4MVSjvf9+WM8+ndbV\nSGaCaA/E5vmSaFus7wDfMbN/mdmbZjYkZl+2mRVF2/8z3guY2ZjomKI1a9bUbvQiklYaNQqTa86f\nH6phSksTJSWpjqxmSsc0DB8e1tF4/fW6GdOwJ1LdSL0P0AU4ARgJ3G9mLaN9naI5ys8DJprZ4RVP\ndvdJ7p7v7vnt2rWrq5hFJIUOPzzMPXT33WWliQcfTP/SRLwxDUVFYd2MdJXMBLESODTmeYdoW6wS\n4Cl33+buHwEfEBIG7r4yul8GvAL0S2KsIpJBGjWCq64KpYk+feDii8NYgZUVv2HSRMUxDYsX1/2Y\nhj2RzAQxB+hiZp3NrAkwAqjYG+lJQukBM2tLqHJaZmatzGzfmO3HAouSGKuIZKDS0sRdd4X7Hj3g\noYfSpzTx+edw4YVlYxr++c8wpqFDh1RHlpikJQh33w5cCTwPLAYec/eFZnazmZX2SnoeWGdmi4CZ\nwM/cfR3QDSgys+Jo++3urgQhIrtp1Ah+/ONQmujdGy66KPWliZ07w7xuRx4JU6eGNWbefTe0mWSS\nBr8mtYjUHzt3hnET118PTZqEksUFF9Rtz6Di4jCe4c034YQTwhxT3brV3evXVFVrUqe6kVpEpNbE\nliZ69YLRo+HMM8Na2clWcUzDlClhTEM6J4fqKEGISL1zxBHw6qthjMHLL4e2iSlTktM2UXFMwyWX\nhDEN55+fXmMa9oQShIjUS40awdVXhyqfnj1DY/HQobVbmvjoo9DeETum4Q9/SM8xDXtCCUJE6rUu\nXUI309/+NozG7tEjTGmxN6WJrVvhttuge/cwFiMTxjTsCSUIEan3srLCuIPi4pAgLrgAhg2D1atr\nfq3SMQ3jxoUR0ZkypmFPKEGISIPRpUtom7jzzrCKXY8e8MgjiZUmYsc0fPNNGNPw+OOZM6ZhTyhB\niEiDkpUF11wTShPduoXG5P/8z8pLExXHNNxwQ2aOadgTShAi0iB95zuh/eA3v4EXXgilicLC8qWJ\n4mIYOBAuuywMwisuDvMp5eSkLu66pAQhIg1WVlYYuzBvXighjBoFZ50FS5fCtdeGMQ1Ll4YusjNn\nZvaYhj1RD5tVRERqpmvXMMPqxIlhQr2//z1sv+yy0FupdevUxpcqKkGIiBBKE9deG0oTY8eWjWlo\nqMkBVIIQESmna9ewgI+oBCEiIpVQghARkbiUIEREJC4lCBERiUsJQkRE4lKCEBGRuJQgREQkLiUI\nERGJyzwZa/ClgJmtAT7ei0u0BdbWUjjJlkmxQmbFm0mxQmbFm0mxQmbFuzexdnL3dvF21JsEsbfM\nrMjd81MdRyIyKVbIrHgzKVbIrHgzKVbIrHiTFauqmEREJC4lCBERiUsJosykVAdQA5kUK2RWvJkU\nK2RWvJkUK2RWvEmJVW0QIiISl0oQIiISlxKEiIjE1eAThJk9YGafm9m7qY6lOmZ2qJnNNLNFZrbQ\nzK5OdUyVMbNsM3vLzIqjWP9fqmOqjpllmdk7ZvaPVMdSHTNbbmYLzGyemRWlOp7qmFlLM3vczN4z\ns8VmdkyqY4rHzLpGn2np7Ssz+0mq46qKmV0T/R9718ymmll2rV27obdBmNnxwCZgirv3THU8VTGz\ng4GD3f1tM9sfmAv8p7svSnFouzEzA5q6+yYzawzMBq529zdTHFqlzOynQD7Q3N3PSHU8VTGz5UC+\nu2fEQC4zexh4zd0nm1kTIMfdv0x1XFUxsyxgJTDA3fdmEG7SmFl7wv+t7u7+tZk9Bjzj7g/VxvUb\nfAnC3WcBX6Q6jkS4+2p3fzt6vBFYDLRPbVTxebApeto4uqXtrxEz6wB8H5ic6ljqGzNrARwP/AnA\n3beme3KIDAY+TNfkEGMfYD8z2wfIAVbV1oUbfILIVGaWC/QD/p3aSCoXVdnMAz4HXnT3tI0VmAj8\nHNiZ6kAS5MALZjbXzMakOphqdAbWAA9GVXiTzaxpqoNKwAhgaqqDqIq7rwR+DawAVgMb3P2F2rq+\nEkQGMrNmwHTgJ+7+VarjqYy773D3vkAHoL+ZpWUVnpmdAXzu7nNTHUsNDHT3POA04IqoqjRd7QPk\nAfe5ez9gM3B9akOqWlQNNhT4a6pjqYqZtQKGEZLwIUBTMxtVW9dXgsgwUX3+dKDQ3f+W6ngSEVUn\nzASGpDqWShwLDI3q9acBJ5rZI6kNqWrRL0fc/XPgCaB/aiOqUglQElOCfJyQMNLZacDb7v5ZqgOp\nxknAR+6+xt23AX8D/qO2Lq4EkUGiht8/AYvd/c5Ux1MVM2tnZi2jx/sBJwPvpTaq+Nz9F+7ewd1z\nCdUKL7t7rf0Kq21m1jTqpEBUVXMKkLa98Nz9U+ATM+sabRoMpF3HigpGkubVS5EVwNFmlhN9Pwwm\ntE3WigafIMxsKvAG0NXMSszsR6mOqQrHAucTfuGWdsM7PdVBVeJgYKaZzQfmENog0r77aIY4EJht\nZsXAW8A/3f25FMdUnauAwujfQ1/gthTHU6ko6Z5M+DWe1qJS2ePA28ACwnd6rU270eC7uYqISHwN\nvgQhIiLxKUGIiEhcShAiIhKXEoSIiMSlBCEiInEpQYhUw8x2VJjhs9ZGAZtZbibMJCwN0z6pDkAk\nA3wdTRki0qCoBCGyh6I1Ge6I1mV4y8yOiLbnmtnLZjbfzF4ys47R9gPN7IlojYxiMyudEiHLzO6P\n5vR/IRp5jpn9OFr7Y76ZTUvR25QGTAlCpHr7VahiOjdm3wZ37wX8jjAjLMA9wMPu3hsoBO6Ott8N\nvOrufQhzES2MtncB7nX3HsCXwA+i7dcD/aLrjE3WmxOpjEZSi1TDzDa5e7M425cDJ7r7smgSxU/d\nvY2ZrSUs7LQt2r7a3dua2Rqgg7t/G3ONXMI0JF2i5/8NNHb3W83sOcJiVk8CT8asryFSJ1SCENk7\nXsnjmvg25vEOytoGvw/cSyhtzIkWhBGpM0oQInvn3Jj7N6LHrxNmhQUoAF6LHr8EXA67FlNqUdlF\nzawRcKi7zwT+G2gB7FaKEUkm/SIRqd5+0cp4pZ5z99Kurq2iGUq/JUwRDWHm0gfN7GeEldQuirZf\nDUyKZgzeQUgWqyt5zSzgkSiJGHB3hizTKfWI2iBE9lDUBpHv7mtTHYtIMqiKSURE4lIJQkRE4lIJ\nQkRE4lKCEBGRuJQgREQkLiUIERGJSwlCRETi+v+N5JPfuQOLegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cb = EarlyStopping(monitor='val_acc', mode='auto', patience=5)\n",
    "glove_history = model_glove.fit(Xtrain,Ytrain,epochs=20,\n",
    "                                validation_split=0.2, batch_size=32,\n",
    "                                verbose=1,workers = 3, use_multiprocessing=True, callbacks = [cb])\n",
    "plotting_history(glove_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "Hd0d_OI2ZZR4",
    "outputId": "093b101c-8844-4312-bced-6dabcdf1054a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37       100\n",
      "           1       0.20      0.04      0.06        27\n",
      "           2       0.30      0.17      0.21        78\n",
      "           3       0.09      0.03      0.04        35\n",
      "           4       0.81      0.91      0.86       620\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.35      0.30      0.31       860\n",
      "weighted avg       0.66      0.72      0.68       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Ytest, np.argmax(model_glove.predict(Categorical_test), axis =1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8pUtggKOofp"
   },
   "source": [
    "#### LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eexKgrgmHTtw",
    "outputId": "ff9e0893-2473-472b-ca1f-56e3cc4cd133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 162, 100)          1821800   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 126)               114408    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 635       \n",
      "=================================================================\n",
      "Total params: 1,936,843\n",
      "Trainable params: 1,936,843\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "10592/10592 [==============================] - 87s 8ms/step - loss: 0.9726 - acc: 0.6781 - val_loss: 0.8794 - val_acc: 0.7028\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 86s 8ms/step - loss: 0.6939 - acc: 0.7610 - val_loss: 0.9348 - val_acc: 0.7062\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 86s 8ms/step - loss: 0.4656 - acc: 0.8373 - val_loss: 1.0291 - val_acc: 0.6647\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 87s 8ms/step - loss: 0.2957 - acc: 0.9007 - val_loss: 1.2461 - val_acc: 0.6681\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 86s 8ms/step - loss: 0.1820 - acc: 0.9433 - val_loss: 1.3905 - val_acc: 0.6673\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 85s 8ms/step - loss: 0.1181 - acc: 0.9652 - val_loss: 1.6103 - val_acc: 0.6416\n",
      "Epoch 7/20\n",
      "10592/10592 [==============================] - 85s 8ms/step - loss: 0.0838 - acc: 0.9747 - val_loss: 1.7729 - val_acc: 0.6360\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.06      0.07        35\n",
      "           1       0.30      0.35      0.32       100\n",
      "           2       0.21      0.17      0.19        78\n",
      "           3       0.21      0.30      0.24        27\n",
      "           4       0.82      0.81      0.82       620\n",
      "\n",
      "    accuracy                           0.65       860\n",
      "   macro avg       0.32      0.34      0.33       860\n",
      "weighted avg       0.65      0.65      0.65       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model():\n",
    "  model=keras.Sequential()\n",
    "  model.add(keras.layers.Embedding(VOCAB_SIZE, 100, input_length=EMBED_SIZE))\n",
    "  #model.add(GlobalAveragePooling1DMasked())\n",
    "  model.add(keras.layers.LSTM(126))\n",
    "  model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "  model.summary()\n",
    "  model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "lstm_model = LSTM_model()\n",
    "lstm_history = lstm_model.fit(Xtrain,Ytrain,epochs=20,\n",
    "                              validation_split=0.2, batch_size=32,\n",
    "                              verbose=1,workers = 3, use_multiprocessing=True, callbacks = [cb])\n",
    "\n",
    "print(classification_report(Ytest, np.argmax(lstm_model.predict(Categorical_test), axis =1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGwbKmAUOxKJ"
   },
   "source": [
    "#### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mwOodD49O0RZ",
    "outputId": "6776325b-210a-43f2-f96e-0ea3405db2d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 162, 100)          1821800   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 324)               340848    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 1625      \n",
      "=================================================================\n",
      "Total params: 2,164,273\n",
      "Trainable params: 2,164,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/20\n",
      "10592/10592 [==============================] - 190s 18ms/step - loss: 0.9593 - acc: 0.6825 - val_loss: 0.8711 - val_acc: 0.7081\n",
      "Epoch 2/20\n",
      "10592/10592 [==============================] - 189s 18ms/step - loss: 0.6953 - acc: 0.7635 - val_loss: 0.9125 - val_acc: 0.7017\n",
      "Epoch 3/20\n",
      "10592/10592 [==============================] - 188s 18ms/step - loss: 0.4860 - acc: 0.8309 - val_loss: 1.0236 - val_acc: 0.6820\n",
      "Epoch 4/20\n",
      "10592/10592 [==============================] - 190s 18ms/step - loss: 0.3282 - acc: 0.8911 - val_loss: 1.1941 - val_acc: 0.6631\n",
      "Epoch 5/20\n",
      "10592/10592 [==============================] - 189s 18ms/step - loss: 0.2262 - acc: 0.9250 - val_loss: 1.3890 - val_acc: 0.6628\n",
      "Epoch 6/20\n",
      "10592/10592 [==============================] - 189s 18ms/step - loss: 0.1467 - acc: 0.9535 - val_loss: 1.5979 - val_acc: 0.6480\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.14      0.17        35\n",
      "           1       0.39      0.36      0.37       100\n",
      "           2       0.22      0.12      0.15        78\n",
      "           3       0.25      0.37      0.30        27\n",
      "           4       0.81      0.86      0.83       620\n",
      "\n",
      "    accuracy                           0.69       860\n",
      "   macro avg       0.38      0.37      0.37       860\n",
      "weighted avg       0.66      0.69      0.67       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "def LSTM_Bidirectional_model():\n",
    "  model=keras.Sequential()\n",
    "  model.add(keras.layers.Embedding(VOCAB_SIZE, 100, input_length=EMBED_SIZE))\n",
    "  #model.add(GlobalAveragePooling1DMasked())\n",
    "  model.add(Bidirectional(keras.layers.LSTM(EMBED_SIZE)))\n",
    "  model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "  model.summary()\n",
    "  model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "Bilstm_model = LSTM_Bidirectional_model()\n",
    "Bilstm_history = Bilstm_model.fit(Xtrain,Ytrain,epochs=20,\n",
    "                              validation_split=0.2, batch_size=32,\n",
    "                              verbose=1,workers = 3, use_multiprocessing=True, callbacks = [cb])\n",
    "\n",
    "print(classification_report(Ytest, np.argmax(Bilstm_model.predict(Categorical_test), axis =1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtMH0rB8jplq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "m8wT9qX1j5gw",
    "x7ngiBE1HJcB",
    "D8tQcGLnojqr",
    "FjawbSOyooRT",
    "4FHILG1HsZ8I",
    "6wTTS0iYWvw9",
    "n8pUtggKOofp",
    "AGwbKmAUOxKJ"
   ],
   "name": "NLP_ass1D.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
